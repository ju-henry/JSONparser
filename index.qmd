---
title: "JSON parser speed comparison"
format: html
bibliography: references.bib
---

```{r results='hide', echo=FALSE}
library(ggplot2)
```

## Question

Parsing a few json files in R is often done using e.g. jsonlite. But if the 
number of files grows (to hundreds of thousands or millions), are there faster
tools or ways to parse them?  

## Tools

Here I compare three tools:  

- [**jsonlite**](https://cran.r-project.org/web/packages/jsonlite/index.html) [@jsonlite]
- [**jq**](https://jqlang.org/) [@jq]
- [**simdjson**](https://simdjson.org/) [@simdjson]

The comparison is done inside R scripts. **jsonlite** is an R package and both 
**jq** and **simdjson** have bindings for R - **jqr** and **RcppSimdJson**. 

The GNU utility **time** [@gnutime] is used to measure elapsed time and CPU usage.

## Experiments 

I run experiments using public X (Twitter) data in three scenarios:  

- **base**: 100 lightweight (~4kB) jsons (one json represents infos about a single tweet)
- **heavier**: 100 heavier (~1MB) jsons (where random entries have been added to the files of the base case)
- **base100**: 10'000 lightweight files (using 100 copies of each file in the base case) 

For all jsons, I am interested in extracting the same 21 fields (e.g. *created_at*, *user.name*,...) 
and write a final csv file to disk. This matters since both **jq** and **simdjson** 
offer the possibility to query specific entries instead of parsing entire files.
The fields I am interested in are sometimes nested and 6 of them require the application of a 
function to get the number of characters or to get the number of entries (nchar() or length(), in R). 

## Results

```{r results='hide', eval=F, echo=F}
system("./get_results.sh")
```

```{r echo=F}
df <- read.csv("~/projets/parser/results_logs.txt", header=FALSE)
df$V1 <- signif(as.numeric(substr(df$V1, 3, nchar(df$V1))), digits = 2)
df$V3 <- gsub(pattern = "parse_|\\.R", replacement = "", x = df$V3)
names(df) <- c("time", "CPU", "tool", "case")
```

```{r echo=F}
ggplot(df, aes(x = case, y = time, colour = tool)) + 
  geom_point(size = 3) + 
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, ceiling(max(df$time)), by = 1)) + 
  theme(panel.grid.major.y = element_line(color = "grey"))
```


