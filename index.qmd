---
title: "JSON parser speed comparison"
format: 
  html:
    css: style.css
bibliography: references.bib
---

```{r results='hide', echo=FALSE}
library(ggplot2)
```
<br>

## Question

::: {.justify}

Parsing a few json files in R is often done using e.g. **jsonlite**. But if the 
number of files grows (to hundreds of thousands or millions), are there faster
tools or ways to parse them?  

:::

<br>

## Tools

::: {.justify}

Here I compare three tools:  

- [**jsonlite**](https://cran.r-project.org/web/packages/jsonlite/index.html) [@jsonlite]
- [**jq**](https://jqlang.org/) [@jq]
- [**simdjson**](https://simdjson.org/) [@simdjson]

**jsonlite** is an R package while **jq** and **simdjson** have bindings for R:  

- [**jqr**](https://cran.r-project.org/web/packages/jqr/index.html) [@jqr]
- [**RcppSimdJson**](https://cran.r-project.org/web/packages/RcppSimdJson/index.html) [@rcppsimdjson]

To add some elements for the comparison, I also use **jq** as a stand-alone 
program (from the shell) as well as a Python binding for **simdjson**:  

- [**pysimdjson**](https://pysimdjson.tkte.ch/index.html) [@pysimdjson]

The GNU utility **time** [@gnutime] is used to measure elapsed wall-clock time.

:::

<br>

## Experiments 

::: {.justify}

I run experiments using public X (Twitter) data in three scenarios:  

- **base**: 100 lightweight (~4kB) jsons (one json represents infos about a single tweet).
- **heavy**: 100 heavier (~1MB) jsons (where entries have been duplicated in files of the base case to reach ~1MB each).
- **many**: ~25'000 lightweight files (~100MB overall) using many copies of each file in the base case. 

For all jsons, I am interested in extracting the same 21 fields (e.g. *created_at*, *user.name*,...) 
and write a final csv file to disk. This matters since both **jq** and **simdjson** 
offer the possibility to query specific entries directly in the parse command.
The fields I am interested in are sometimes nested and 6 of them require the application of a 
function to get the number of characters or to get the number of entries (nchar() or length(), in R). 

In order to get stable results, all experiments are run 5 times and the median value is used.

:::

<br>

## Results

### Graph

```{r results='hide', eval=F, echo=F}
system("./get_results.sh")
```

```{r echo=F}
res_files <- paste0("results_logs_", 1:5, ".csv")
read_csv_header <- function(f) {
  
  values <- read.csv(f, header = FALSE)[, 1]
  values <- sub(pattern = "^0:", "", values)
  values <- as.numeric(values)
  
  return(values)
}
A <- sapply(res_files, read_csv_header)
times <- apply(A, 1, median)

df <- read.csv(res_files[1], header=FALSE)
df$V1 <- times
df$V1 <- signif(df$V1, digits = 2)
df$V3 <- gsub(pattern = "parse_|\\.R|\\.py|\\.sh", replacement = "", x = df$V3)
names(df) <- c("time", "CPU", "tool", "case")
```

```{r echo=F}
ggplot(df, aes(x = case, y = time, colour = tool)) + 
  geom_point(size = 3) + 
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, ceiling(max(df$time)), by = 1)) + 
  theme(panel.grid.major.y = element_line(color = "grey"))
```

### Table

```{r echo=FALSE}
df <- df[order(df$case, df$tool),]
B <- matrix(data = df$time, nrow = length(unique(df$tool)), ncol = length(unique(df$case)), byrow = F)
dfB <- data.frame(B, row.names = df$tool[1:nrow(B)])
names(dfB) <- unique(df$case)
print(dfB)
```

### Observations

::: {.justify}

-  **Pysimdjson** dominates other tools in the last two cases and comes second to **jq** in the base case. Contrarily to other tools / bindings, it is barely affected by the size of the files (second case). **Pysimdjson** also does better than its R counterpart (**rcppsimdjson**).
- As one could expect, **jq** fares better than its R binding (**jqr**).
- R-based tools seem to suffer in the last case (maybe because of the long *for* loop).

:::

<br>

## Conclusion

**pysimdjson** seems to be the obvious answer, being faster than other tools when json files are heavy or many.

## Reproductibility
